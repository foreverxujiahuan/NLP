# 决策树

决策树是一种可解释性很强的算法，是一种非常经典的机器学习算法模型。后期的许多算法都是基于决策树而来，例如Xgboost、随机森林等。因此掌握好决策树是很有必要的。

在将具体的决策树算法模型之前，我们需要先对决策树有一个大概的了解。首先它是一种树状的算法模型，以分类问题为例，它会根据特征之间的不同值不断的做出选择，最后求出叶子结点.在决策树中，叶子节点上标签，除了叶子结点之外的节点则是特征.

这是一个小伙子在考虑要不要去相亲的选择问题.他会先后根据这个女生白不白、富不富、美不美来决定是否要去参加相亲.中间的这些则是数据的特征，最后的去、不去、犹豫则是标签。

经典的决策树模型有ID3、C4.5、CART等

## ID3

ID3算法的核心思想就是选出当前信息增益最大的特征进行分裂。

熵是指事物不确定性的程度，熵越高，不确定性越大，熵越低，不确定性越低.

对于特征A来说，信息增益表示的是得知特征A使得样本不确定性减少的程度。

对于一个数据集D来说，它的信息熵为:

$$ H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|} $$

其中 $C_k$ 表示集合 $D$ 中属于第k类样本的子集

举个例子，对于一个二分类问题来说，如果 $D$ 个样本中全是正样本，那么这个数据集的信息熵是最低的，就是0.假如说绝大多数是正样本极少数是负样本，那么它的信息熵也是比较低的.在正负样本各一半的情况下，它的信息熵则是最高的.

了解了信息熵之后，我们再来看信息增益。前面讲过，信息增益是指得知了特征A之后，对于信息不确定性减少的程度。

针对某个特征 $A$ ，数据集 $D$ 的条件熵 $H(D|A)$ 为

$$ H(D|A) = -\sum_{i=1}^{n}\frac{D_i}{D}(\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}) $$

最后 $A$ 对于 $D$ 信息增益则是

$$ Gain(D, A) = H(D) - H(D|A) $$

ID3就是简单的考虑信息增益，每次都选择信息增益最大的特征来构建树，是一种思想比较简单的算法.

## C4.5

前面我们知道ID3会根据信息增益来进行特征选择分裂，和ID3不同的是，C4.5采用信息增益率来作为选择特征分裂的依据.

$$ Gain_{ratio}(D, A) = \frac{Gain(D, A)}{H_A(D)} $$

除了使用信息增益率来替代信息增益之外，它还引入了悲观剪枝策略进行后剪枝

对于离散的特征，C4.5会进行离散化等

## CART

CART树采用了基尼指数作为划分的标准

基尼指数代表了模型的不纯度，基尼指数越小，不纯度越低，代表模型越好

$$ Gini(D) = \sum_{k=1}^{K}\frac{|C_k|}{|D|}(1 - \frac{|C_k|}{|D|}) $$

$$ Gini(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}Gina(D_i) $$

# 机器学习中的评价指标

## 分类任务中常见的评价指标

#### 1.准确率

准确率很好理解，给定n个样本，算法模型去预测这n个样本的label,其中有m个样本是预测对了的，那么

$$ acc = \frac{m}{n} $$ 

    
#### 2.精确率

精确率主要是衡量模型对于正样本识别的把握的值。考虑一个二分类问题，总共有n个样本，模型对于这n个样本中预测为正样本的数据中，有TP个是预测正确的，FP个是预测错误的，那么

$$ precision=\frac{TP}{TP+FP} $$

#### 3.召回率

召回率主要是衡量模型对于正样本的召回能力。考虑一个二分类模型，总共有n个样本，其中有p个正样本，其中模型预测为正样本且预测正确的有TP个，是正样本但是模型没有预测出来的有FN个，那么

$$ recall=\frac{TP}{TP+FN} $$

#### 4.F1

一般我们会用F1来综合衡量一个分类模型的好坏。它平衡了精确率和召回率两个之间的值，计算出一个类似于“平均”的值

$$ F1 = \frac{2*Percision*Recall}{Percision+Recall}$$

## 回归任务中常见的评价指标



对于n个样本，样本的真实值

$$ y=[y_1,y_2,y_3,...,y_n]$$ 

算法模型预测为


$$\hat y = [\hat y_1, \hat y_2, \hat y_3, ..., \hat y_n] $$

#### 5.MAE

$$ MAE = \frac{1}{n}\sum_1^n|y_1 - \hat y_1|$$
    

#### 6.MSE
    
$$ MAE = \frac{1}{n}\sum_1^n(y_1 - \hat y_1)^2$$


## 交叉验证

很多人在评估算法的时候会用到交叉验证。

以五折交叉验证为例，指的是每次都取数据的80%作为训练集、剩下的20%作为验证集，一共取五次，使得每个样本都当过验证集。最后取五次实验的平均结果作为最终成绩。


## 训练集、验证集与测试集的区别

很多同学在学习的时候搞不清楚验证集和测试集的区别。
一般我们认为训练集和验证集是一起的，是用来让你调参的，等你调的差不多了，可以使用调好的模型去测试集上测试，但是你不能用测试集的结果继续调整。

就好比高中的时候，训练集是你平时的作业，验证集是你的模拟考，而测试集是高考。你之前都是在用训练集和验证集不断的优化自己，最后究竟优化的怎么样，还得是测试上的成绩说了算。